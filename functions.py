{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import scipy\n",
    "from scipy.special import loggamma, polygamma, zeta\n",
    "\n",
    "## estimate beta\n",
    "\n",
    "'''\n",
    "beta is the hyperparameter defining the (uniform) Dirichlet prior, \n",
    "which is assumed as the generative model for observed data.\n",
    "\n",
    "beta is inferred from observations ns: \n",
    "ns = list of counts \n",
    "\n",
    "ex: K = 10; N = 52;\n",
    "ns = {n_l:l=1,...K} = [0,30,1,2,0,0,9,6,3,1] -> beta\n",
    "'''\n",
    "\n",
    "def Derivative(ns, beta):\n",
    "    k=len(ns)\n",
    "    N=sum(ns) \n",
    "    #print(k,N)\n",
    "    s1=0\n",
    "    for n in ns:\n",
    "        for m in range(n):\n",
    "            d=m+beta\n",
    "            s1+=1./d\n",
    "    s2=0\n",
    "    for n in range(N):\n",
    "        d=n+k*beta\n",
    "        s2+=k/d\n",
    "    q=s1-s2\n",
    "    return q\n",
    "\n",
    "def DerivativeNSB(ns, beta):\n",
    "    k=len(ns)\n",
    "    N=sum(ns) \n",
    "    #print(k,N)\n",
    "    s1=0\n",
    "    for n in ns:\n",
    "        for m in range(n):\n",
    "            d=m+beta\n",
    "            s1+=1./d\n",
    "    s2=0\n",
    "    for n in range(N):\n",
    "        d=n+k*beta\n",
    "        s2+=k/d\n",
    "    q=s1-s2\n",
    "    d2sbar=k*k*polygamma(2,k*beta+1)-polygamma(2,beta+1)\n",
    "    dsbar=k*polygamma(1,k*beta+1)-polygamma(1,beta+1)\n",
    "    #print(beta,q,d2sbar)\n",
    "    return q+d2sbar/dsbar\n",
    "\n",
    "def beta_star(ns, niter=10): \n",
    "    beta=[0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000,1000000,10000000]\n",
    "    for iter in range (niter):\n",
    "        if iter==0: betas=beta\n",
    "        b=betas[0]\n",
    "        #print(b)    \n",
    "        q=Derivative(ns,b)\n",
    "        s=-1\n",
    "        if q>0: s=1\n",
    "        #print(iter,q,s)\n",
    "        for b in betas:\n",
    "            q=Derivative(ns,b)\n",
    "            #print(b,q)\n",
    "            if q*s<0:\n",
    "            #change of sign\n",
    "                if iter==0:\n",
    "                    betas=[b/10+n*b/10 for n in range(11)]\n",
    "                    #print('i0 change',b)\n",
    "                    break\n",
    "                else:\n",
    "                    db=betas[1]-betas[0]\n",
    "                    betas=[b-db+db/10*n for n in range(11)]\n",
    "                    #print('change',b,q)\n",
    "                    break\n",
    "\n",
    "    # If no solution was found, select among extremes\n",
    "    if b == beta[-1]:\n",
    "        if Derivative(ns, b) < 0:\n",
    "            b = beta[0]\n",
    "    # Done\n",
    "    return b, Derivative(ns,b)\n",
    "\n",
    "def beta_starNSB(ns, niter=10):\n",
    "    beta=[0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000,1000000,10000000]\n",
    "    for iter in range (niter):\n",
    "        if iter==0: betas=beta\n",
    "        b=betas[0]\n",
    "        #print(b)    \n",
    "        q=DerivativeNSB(ns,b)\n",
    "        s=-1\n",
    "        if q>0: s=1\n",
    "        #print('iter',iter,q,s)\n",
    "        for b in betas:\n",
    "            q=DerivativeNSB(ns,b)\n",
    "            #print(b,q)\n",
    "            if q*s<0:\n",
    "            #change of sign\n",
    "                if iter==0:\n",
    "                    betas=[b/10+n*b/10 for n in range(11)]\n",
    "                    #print('i0 change',b)\n",
    "                    break\n",
    "                else:\n",
    "                    db=betas[1]-betas[0]\n",
    "                    betas=[b-db+db/10*n for n in range(11)]\n",
    "                    #print('change',b,q)\n",
    "                    break\n",
    "\n",
    "    # If no solution was found, select among extremes\n",
    "    if b == beta[-1]:\n",
    "        if DerivativeNSB(ns, b) < 0:\n",
    "            b = beta[0]\n",
    "    # Done\n",
    "    return b, DerivativeNSB(ns,b)\n",
    "\n",
    "\n",
    "## functions useful for simulations (and to replicate results in the paper)\n",
    "\n",
    "def generate_data(K=100,N = 100, beta=1, niter = 20, add_zeroes=False):\n",
    "    \n",
    "    '''\n",
    "    given beta, the hyperparameter of a uniform Dirichlet distribution with K categories, the outputs are:\n",
    "    \n",
    "    1. true_rhos: a categorical distribution sampled from the Dirichlet \n",
    "    2. ns: a multinomial sampling from the distribution true_rhos (a list, where N is the sum of its elements, i.e. the total number of counts)\n",
    "    3. the estimated beta from counts: \n",
    "            3.1 bstar: from uniform hyperprior\n",
    "            3.2 bstarNSB: from NSB hyperprior\n",
    "            \n",
    "    if add_zeroes=True, half bins of true_rhos are set zero\n",
    "    \n",
    "    '''\n",
    "\n",
    "    true_rhos = list(np.random.dirichlet(beta*np.ones(K)))\n",
    "    \n",
    "    if add_zeroes == True:\n",
    "        true_rhos = list(true_rhos[:K//2]) + [0]*(K-K//2)\n",
    "        true_rhos/=sum(true_rhos)\n",
    "        \n",
    "    ns = list(np.random.multinomial(N, true_rhos)) \n",
    "    \n",
    "    bstar, gap = beta_star(ns, niter)\n",
    "    bstarNSB, gapNSB = beta_starNSB(ns, niter)\n",
    "    \n",
    "    # Done\n",
    "    return ns, true_rhos, bstar, gap, bstarNSB, gapNSB\n",
    "\n",
    "\n",
    "def generate_zipf(K=100,N = 100, a = 1.1, niter = 20):\n",
    "    \n",
    "    '''\n",
    "    given a, the exponent of a Zipf's distribution, the outputs are:\n",
    "    \n",
    "    1. true_rhos: a discretized version of a Zipf's distribution with K categories\n",
    "    2. ns: a multinomial sampling from the distribution true_rhos (a list, where N is the sum of its elements, i.e. the total number of counts)\n",
    "    3. the estimated beta from counts: \n",
    "            3.1 bstar: from uniform hyperprior\n",
    "            3.2 bstarNSB: from NSB hyperprior\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    bins = np.arange(1, K+1)\n",
    "    \n",
    "    # define zipf with K bins\n",
    "    true_rhos = ( bins**(-a*1.0)) / sum([k**(-a*1.0) for k in bins] )\n",
    "    true_rhos /= true_rhos.sum() # normalize\n",
    "    zipf_dist = stats.rv_discrete(name='zipf_dist', values=(bins, true_rhos))\n",
    "    \n",
    "    # sample multinomial observations from zipf\n",
    "    ns = zipf_dist.rvs(size=N)\n",
    "    hist, _ = np.histogram(ns,np.arange(1, K+2))\n",
    "    \n",
    "    # estimate beta* from observations\n",
    "    bstar, gap = beta_star(hist, niter)\n",
    "    bstarNSB, gapNSB = beta_starNSB(hist, niter)\n",
    "      \n",
    "    # Done\n",
    "    return hist, true_rhos, bstar, gap, bstarNSB, gapNSB\n",
    "\n",
    "\n",
    "#%% INFORMATION THEORETIC QUANTITIES:\n",
    "\n",
    "# SHANNON ENTROPY OF THE DISTRIBUTION\n",
    "\n",
    "def entropy(rhos):\n",
    "    '''\n",
    "    Shannon entropy of a distribution\n",
    "    '''\n",
    "    \n",
    "    rhos_pos = [r for r in rhos if r != 0]\n",
    "    return -np.sum(rhos_pos * np.log(rhos_pos) / np.log(len(rhos)))\n",
    "\n",
    "def zipf_entropy(K,a, norm = True):\n",
    "    '''\n",
    "    exact formula of the Shannon entropy of a Zipf's distribution with exponent a\n",
    "    '''\n",
    "    n = 1\n",
    "    if norm == True:\n",
    "        n = np.log(K+1)        \n",
    "    harm_num = sum([k**-a for k in range(1,K+1)])\n",
    "    return (a / harm_num * sum([np.log(k) / (k**a) for k in range(1,K+1)]) + np.log(harm_num)) / n\n",
    "\n",
    "# ESTIMATED SHANNON ENTROPIES FROM COUNTS\n",
    "\n",
    "def ml_entropy(ns):\n",
    "    '''\n",
    "    Shannon Entropy estimated from counts ns, \n",
    "    using the maximum-likelihood estimator for the true distribution rho: rho_i = n_i/N\n",
    "    '''\n",
    "    K = len(ns)\n",
    "    N = np.sum(ns)\n",
    "    mlrhos = [n / N for n in ns if n != 0]\n",
    "    return -np.sum(mlrhos * np.log(mlrhos) / np.log(K))\n",
    "                  \n",
    "\n",
    "def ww_entropy(ns,beta):\n",
    "    '''\n",
    "    Wolpert-Wolf Shannon Entropy estimated from counts ns, given the value of the hyperparameter beta\n",
    "    '''\n",
    "    ns = np.array(ns)\n",
    "    K = len(ns)\n",
    "    N = np.sum(ns)\n",
    "    ww_ent = polygamma(0,N+K*beta+1) - sum((ns+beta) / (N+K*beta) * polygamma(0,ns+beta+1))\n",
    "    return ww_ent/np.log(K)\n",
    "    \n",
    "\n",
    "def bay_entropy(ns, beta):\n",
    "    '''\n",
    "    Shannon entropy estimated from counts ns, given the value of the hyperparameter beta,\n",
    "    using the Bayesian (Laplace) formula:\n",
    "    rho_i = (n_i + beta) / (N + K*beta)\n",
    "    '''\n",
    "    K = len(ns)\n",
    "    N = np.sum(ns)\n",
    "    bayrhos = [(n + beta) / (N + K*beta) for n in ns]\n",
    "    return -np.sum(bayrhos * np.log(bayrhos) / np.log(K))\n",
    "\n",
    "# hausser-strimmer empirical estimator\n",
    "def lambda_hs(ns,t=1/100):\n",
    "    K = len(ns)\n",
    "    N = sum(ns)\n",
    "    return (1-sum([(n/N)**2 for n in ns]))/((N-1)*sum([(t-n/N)**2 for n in ns]))\n",
    "\n",
    "\n",
    "def hs_entropy(ns):\n",
    "    '''\n",
    "    Hausser-Strimmer Shannon entropy empirical estimator from counts ns\n",
    "    '''\n",
    "    K = len(ns)\n",
    "    N = sum(ns)\n",
    "    t = 1/K\n",
    "    l = lambda_hs(ns,t)\n",
    "    rhos = [l*t+(1-l)*n/N for n in ns]\n",
    "    return entropy(rhos)\n",
    "\n",
    "\n",
    "# KULLBACK-LEIBLER DIVERGENCE BETWEEN TWO TRUE DISTRIBUTIONS\n",
    "\n",
    "def kl(rhos,sigmas):\n",
    "    return scipy.stats.entropy(rhos, sigmas, axis=0) \n",
    "\n",
    "\n",
    "# ESTIMATED KL FROM COUNTS:\n",
    "\n",
    "# CASE 1: KL between the true disribution and its sampling \n",
    "\n",
    "def bay_kl_same(rhos,ns, beta):\n",
    "    K = len(rhos)\n",
    "    N = np.sum(ns);\n",
    "    bayq = [(n + beta) / (N + K*beta) for n in ns]\n",
    "    return kl(rhos,bayq) # scipy.stats.entropy(rhos, bayq, axis=0) # \n",
    "\n",
    "def hs_kl_same(rhos,multi):\n",
    "    K = len(multi)\n",
    "    N = sum(multi)\n",
    "    t = 1/K\n",
    "    l = lambda_hs(multi,t)\n",
    "    rhos_multi = [l*t+(1-l)*n/N for n in multi]\n",
    "    return scipy.stats.entropy(rhos, rhos_multi, axis=0) # kl(rhos, rhos_multi)\n",
    "\n",
    "# CASE 2: KL between two different estimated (from counts) distributions \n",
    "\n",
    "def bay_kl(nsp,nsq, betap, betaq):\n",
    "    '''\n",
    "    kl estimated from counts using the Bayesian Laplace's formula: rho_i = (n_i + beta) / (N + K*beta)\n",
    "    '''\n",
    "    K = len(nsp)\n",
    "    Np = np.sum(nsp); Nq = np.sum(nsq);\n",
    "    bayp = [(n + betap) / (Np + K*betap) for n in nsp]\n",
    "    bayq = [(n + betaq) / (Nq + K*betaq) for n in nsq]\n",
    "    return kl(bayp,bayq)\n",
    "\n",
    "def hs_kl(nsp,nsq):\n",
    "    '''\n",
    "    Hausser-Strimmer KL empirical estimator from counts\n",
    "    '''\n",
    "    K = len(nsp)\n",
    "    Np, Nq = np.sum(nsp), np.sum(nsq);\n",
    "    t = 1/K\n",
    "    lp, lq = lambda_hs(nsp,t), lambda_hs(nsq,t)\n",
    "    rhosp = [lp*t+(1-lp)*n/Np for n in nsp]\n",
    "    rhosq = [lq*t+(1-lq)*n/Nq for n in nsq]\n",
    "    return kl(rhosp,rhosq)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
